

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dire_rapids.dire_pytorch_memory_efficient &mdash; dire-rapids 0.2.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=9edc463e" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=938c9ccc"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >

          
          
          <a href="../../index.html" class="icon icon-home">
            dire-rapids
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/modules.html">dire_rapids</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">dire-rapids</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dire_rapids.dire_pytorch_memory_efficient</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dire_rapids.dire_pytorch_memory_efficient</h1><div class="highlight"><pre>
<span></span><span class="c1"># dire_pytorch_memory_efficient.py</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Memory-efficient PyTorch/PyKeOps backend for DiRe.</span>

<span class="sd">This implementation inherits from DiRePyTorch and overrides specific methods for:</span>
<span class="sd">- FP16 support for memory-efficient k-NN computation</span>
<span class="sd">- Point-by-point attraction force computation to avoid large tensor materialization</span>
<span class="sd">- More aggressive memory management and cache clearing</span>
<span class="sd">- Optional PyKeOps LazyTensors for repulsion when available</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">loguru</span><span class="w"> </span><span class="kn">import</span> <span class="n">logger</span>

<span class="c1"># Import base class and compiled kernels</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.dire_pytorch</span><span class="w"> </span><span class="kn">import</span> <span class="n">DiRePyTorch</span>  <span class="c1"># pylint: disable=cyclic-import</span>

<span class="c1"># PyKeOps for efficient force computations</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">pykeops.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">LazyTensor</span>
    <span class="n">PYKEOPS_AVAILABLE</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">PYKEOPS_AVAILABLE</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;PyKeOps not available. Install with: pip install pykeops&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="DiRePyTorchMemoryEfficient">
<a class="viewcode-back" href="../../api/modules.html#dire_rapids.dire_pytorch_memory_efficient.DiRePyTorchMemoryEfficient">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DiRePyTorchMemoryEfficient</span><span class="p">(</span><span class="n">DiRePyTorch</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Memory-optimized PyTorch implementation of DiRe for large-scale datasets.</span>
<span class="sd">    </span>
<span class="sd">    This class extends DiRePyTorch with enhanced memory management capabilities,</span>
<span class="sd">    making it suitable for processing very large datasets that would otherwise</span>
<span class="sd">    cause out-of-memory errors with the standard implementation.</span>
<span class="sd">    </span>
<span class="sd">    Key Improvements over DiRePyTorch</span>
<span class="sd">    ---------------------------------</span>
<span class="sd">    - **FP16 Support**: Uses half-precision by default for 2x memory reduction</span>
<span class="sd">    - **Dynamic Chunking**: Automatically adjusts chunk sizes based on available memory</span>
<span class="sd">    - **Aggressive Cleanup**: More frequent garbage collection and cache clearing</span>
<span class="sd">    - **PyKeOps Integration**: Optional LazyTensors for memory-efficient exact repulsion</span>
<span class="sd">    - **Memory Monitoring**: Real-time memory usage tracking and warnings</span>
<span class="sd">    - **Point-wise Processing**: Falls back to point-by-point computation when needed</span>
<span class="sd">    </span>
<span class="sd">    Best Use Cases</span>
<span class="sd">    --------------</span>
<span class="sd">    - Datasets with &gt;100K points</span>
<span class="sd">    - High-dimensional data (&gt;500 features) </span>
<span class="sd">    - Memory-constrained environments</span>
<span class="sd">    - Production systems requiring reliable memory usage</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    *args</span>
<span class="sd">        Positional arguments passed to DiRePyTorch parent class.</span>
<span class="sd">    use_fp16 : bool, default=True</span>
<span class="sd">        Enable FP16 precision for memory efficiency (recommended).</span>
<span class="sd">        Provides 2x memory reduction and significant speed improvements.</span>
<span class="sd">    use_pykeops_repulsion : bool, default=True</span>
<span class="sd">        Use PyKeOps LazyTensors for repulsion when beneficial.</span>
<span class="sd">        Automatically disabled if PyKeOps unavailable or dataset too large.</span>
<span class="sd">    pykeops_threshold : int, default=50000</span>
<span class="sd">        Maximum dataset size for PyKeOps all-pairs computation.</span>
<span class="sd">        Above this threshold, random sampling is used instead.</span>
<span class="sd">    memory_fraction : float, default=0.25</span>
<span class="sd">        Fraction of available memory to use for computations.</span>
<span class="sd">        Lower values are more conservative but may be slower.</span>
<span class="sd">    **kwargs</span>
<span class="sd">        Additional keyword arguments passed to DiRePyTorch parent class.</span>
<span class="sd">        Includes: n_components, n_neighbors, init, max_iter_layout, min_dist,</span>
<span class="sd">        spread, cutoff, neg_ratio, verbose, random_state, use_exact_repulsion,</span>
<span class="sd">        metric (custom distance function for k-NN computation).</span>
<span class="sd">        </span>
<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    Memory-efficient processing of large dataset::</span>
<span class="sd">    </span>
<span class="sd">        from dire_rapids import DiRePyTorchMemoryEfficient</span>
<span class="sd">        import numpy as np</span>
<span class="sd">        </span>
<span class="sd">        # Large dataset</span>
<span class="sd">        X = np.random.randn(500000, 512)</span>
<span class="sd">        </span>
<span class="sd">        # Memory-efficient reducer</span>
<span class="sd">        reducer = DiRePyTorchMemoryEfficient(</span>
<span class="sd">            use_fp16=True,</span>
<span class="sd">            memory_fraction=0.3,</span>
<span class="sd">            verbose=True</span>
<span class="sd">        )</span>
<span class="sd">        </span>
<span class="sd">        embedding = reducer.fit_transform(X)</span>
<span class="sd">        </span>
<span class="sd">    Custom memory settings::</span>

<span class="sd">        reducer = DiRePyTorchMemoryEfficient(</span>
<span class="sd">            use_pykeops_repulsion=False,  # Disable PyKeOps</span>
<span class="sd">            memory_fraction=0.15,         # Use less memory</span>
<span class="sd">            pykeops_threshold=20000       # Lower PyKeOps threshold</span>
<span class="sd">        )</span>

<span class="sd">    With custom distance metric::</span>

<span class="sd">        # L1 metric for k-NN with memory efficiency</span>
<span class="sd">        reducer = DiRePyTorchMemoryEfficient(</span>
<span class="sd">            metric=&#39;(x - y).abs().sum(-1)&#39;,</span>
<span class="sd">            n_neighbors=32,</span>
<span class="sd">            use_fp16=True,</span>
<span class="sd">            memory_fraction=0.2</span>
<span class="sd">        )</span>

<span class="sd">        embedding = reducer.fit_transform(X)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
<div class="viewcode-block" id="DiRePyTorchMemoryEfficient.__init__">
<a class="viewcode-back" href="../../api/modules.html#dire_rapids.dire_pytorch_memory_efficient.DiRePyTorchMemoryEfficient.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">use_fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Enable FP16 by default for memory efficiency</span>
        <span class="n">use_pykeops_repulsion</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Use PyKeOps for repulsion when possible</span>
        <span class="n">pykeops_threshold</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>     <span class="c1"># Max points for PyKeOps all-pairs</span>
        <span class="n">memory_fraction</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize memory-efficient DiRe reducer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        *args</span>
<span class="sd">            Positional arguments passed to DiRePyTorch parent class.</span>
<span class="sd">        use_fp16 : bool, default=True</span>
<span class="sd">            Enable FP16 precision for memory efficiency. Provides 2x memory</span>
<span class="sd">            reduction and significant speed improvements on modern GPUs.</span>
<span class="sd">        use_pykeops_repulsion : bool, default=True</span>
<span class="sd">            Use PyKeOps LazyTensors for memory-efficient repulsion computation</span>
<span class="sd">            when dataset size is below pykeops_threshold.</span>
<span class="sd">        pykeops_threshold : int, default=50000</span>
<span class="sd">            Maximum dataset size for PyKeOps all-pairs computation.</span>
<span class="sd">            Above this threshold, random sampling is used instead.</span>
<span class="sd">        memory_fraction : float, default=0.25</span>
<span class="sd">            Fraction of available memory to use for computations.</span>
<span class="sd">            Lower values are more conservative but may be slower.</span>
<span class="sd">        **kwargs</span>
<span class="sd">            Additional keyword arguments passed to DiRePyTorch parent class.</span>
<span class="sd">            See DiRePyTorch documentation for available parameters including:</span>

<span class="sd">            - n_components, n_neighbors, init, max_iter_layout, min_dist, spread</span>
<span class="sd">            - cutoff, neg_ratio, verbose, random_state, use_exact_repulsion</span>
<span class="sd">            - metric: Custom distance metric for k-NN (str, callable, or None)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="c1"># Call parent constructor</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Additional memory-efficient parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_fp16</span> <span class="o">=</span> <span class="n">use_fp16</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_pykeops_repulsion</span> <span class="o">=</span> <span class="n">use_pykeops_repulsion</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pykeops_threshold</span> <span class="o">=</span> <span class="n">pykeops_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_fraction</span> <span class="o">=</span> <span class="n">memory_fraction</span>
        
        <span class="c1"># Log memory-efficient settings</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Memory-efficient mode enabled&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_fp16</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;FP16 enabled for k-NN computation&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_pykeops_repulsion</span> <span class="ow">and</span> <span class="n">PYKEOPS_AVAILABLE</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PyKeOps repulsion enabled (threshold: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pykeops_threshold</span><span class="si">}</span><span class="s2"> points)&quot;</span><span class="p">)</span></div>

    
    <span class="k">def</span><span class="w"> </span><span class="nf">_get_available_memory</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get available system memory in bytes.</span>
<span class="sd">        </span>
<span class="sd">        Private method that queries the available memory on the current device</span>
<span class="sd">        (GPU or CPU) to inform memory-aware chunk sizing decisions.</span>
<span class="sd">        </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Available memory in bytes.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Private method, should not be called directly. Used by _compute_optimal_chunk_size().</span>
<span class="sd">        </span>
<span class="sd">        For CUDA devices, returns free GPU memory.</span>
<span class="sd">        For CPU, returns available system RAM.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Free memory</span>
        <span class="kn">import</span><span class="w"> </span><span class="nn">psutil</span>  <span class="c1"># pylint: disable=import-outside-toplevel</span>
        <span class="k">return</span> <span class="n">psutil</span><span class="o">.</span><span class="n">virtual_memory</span><span class="p">()</span><span class="o">.</span><span class="n">available</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_optimal_chunk_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">operation_type</span><span class="o">=</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute optimal chunk size based on available memory and operation type.</span>
<span class="sd">        </span>
<span class="sd">        This private method dynamically calculates the optimal chunk size for different</span>
<span class="sd">        operations based on available system memory, data characteristics, and the</span>
<span class="sd">        configured memory fraction.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        n_samples : int</span>
<span class="sd">            Total number of samples in the dataset.</span>
<span class="sd">        n_features : int</span>
<span class="sd">            Number of features per sample.</span>
<span class="sd">        operation_type : {&#39;knn&#39;, &#39;repulsion&#39;, &#39;general&#39;}, default=&#39;knn&#39;</span>
<span class="sd">            Type of operation to optimize for:</span>
<span class="sd">            - &#39;knn&#39;: k-nearest neighbors computation</span>
<span class="sd">            - &#39;repulsion&#39;: Repulsion force computation  </span>
<span class="sd">            - &#39;general&#39;: General tensor operations</span>
<span class="sd">        dtype : torch.dtype, default=torch.float32</span>
<span class="sd">            Data type for memory calculations.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Optimal chunk size for the specified operation.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Private method, should not be called directly. Used by _compute_knn() and _compute_forces().</span>
<span class="sd">        </span>
<span class="sd">        The chunk size is bounded between reasonable minimum and maximum values</span>
<span class="sd">        to ensure both memory safety and computational efficiency.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">available_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_available_memory</span><span class="p">()</span>
        <span class="n">usable_memory</span> <span class="o">=</span> <span class="n">available_memory</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_fraction</span>
        
        <span class="n">bytes_per_element</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="mi">4</span>
        
        <span class="k">if</span> <span class="n">operation_type</span> <span class="o">==</span> <span class="s2">&quot;knn&quot;</span><span class="p">:</span>
            <span class="c1"># For k-NN: chunk_size × n_samples × bytes_per_element (distance matrix chunk)</span>
            <span class="n">max_chunk_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">usable_memory</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="n">bytes_per_element</span><span class="p">))</span>
            
        <span class="k">elif</span> <span class="n">operation_type</span> <span class="o">==</span> <span class="s2">&quot;repulsion&quot;</span><span class="p">:</span>
            <span class="c1"># For repulsion: chunk_size × n_neg × n_components × bytes_per_element</span>
            <span class="n">n_neg</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_ratio</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors</span><span class="p">),</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">memory_per_sample</span> <span class="o">=</span> <span class="n">n_neg</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span> <span class="o">*</span> <span class="n">bytes_per_element</span>
            <span class="n">max_chunk_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">usable_memory</span> <span class="o">/</span> <span class="n">memory_per_sample</span><span class="p">)</span>
            
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># general</span>
            <span class="c1"># Conservative estimate: chunk_size × n_features × bytes_per_element</span>
            <span class="n">memory_per_sample</span> <span class="o">=</span> <span class="n">n_features</span> <span class="o">*</span> <span class="n">bytes_per_element</span>
            <span class="n">max_chunk_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">usable_memory</span> <span class="o">/</span> <span class="n">memory_per_sample</span><span class="p">)</span>
        
        <span class="c1"># Apply reasonable bounds</span>
        <span class="n">min_chunk_size</span> <span class="o">=</span> <span class="mi">100</span>
        <span class="n">max_reasonable_chunk_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        
        <span class="n">optimal_chunk_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">min_chunk_size</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_chunk_size</span><span class="p">,</span> <span class="n">max_reasonable_chunk_size</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory-aware chunk sizing for </span><span class="si">{</span><span class="n">operation_type</span><span class="si">}</span><span class="s2">: &quot;</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">optimal_chunk_size</span><span class="si">}</span><span class="s2"> (available: </span><span class="si">{</span><span class="n">available_memory</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB)&quot;</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">optimal_chunk_size</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_knn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_fp16</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute k-nearest neighbors with enhanced memory management.</span>
<span class="sd">        </span>
<span class="sd">        This method overrides the parent implementation with memory-aware chunk</span>
<span class="sd">        sizing, automatic FP16 selection, and aggressive memory cleanup.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : numpy.ndarray</span>
<span class="sd">            Input data of shape (n_samples, n_features).</span>
<span class="sd">        chunk_size : int, optional</span>
<span class="sd">            Size of chunks for processing. If None, automatically computed</span>
<span class="sd">            based on available memory.</span>
<span class="sd">        use_fp16 : bool, optional</span>
<span class="sd">            Use FP16 precision. If None, automatically determined based on</span>
<span class="sd">            data size and GPU capabilities.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Private method, should not be called directly. Used by fit_transform().</span>
<span class="sd">        </span>
<span class="sd">        Enhancements over parent method:</span>
<span class="sd">        - Automatic FP16 selection for large/high-dimensional datasets</span>
<span class="sd">        - Memory-aware chunk size computation</span>
<span class="sd">        - More aggressive memory cleanup after processing</span>
<span class="sd">        </span>
<span class="sd">        Side Effects</span>
<span class="sd">        ------------</span>
<span class="sd">        Sets self._knn_indices and self._knn_distances with computed k-NN graph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">n_dims</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Use instance setting if not explicitly provided</span>
        <span class="k">if</span> <span class="n">use_fp16</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">use_fp16</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_fp16</span>
        
        <span class="c1"># Force FP16 for large/high-dimensional datasets on GPU</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">and</span> <span class="p">(</span><span class="n">n_dims</span> <span class="o">&gt;=</span> <span class="mi">100</span> <span class="ow">or</span> <span class="n">n_samples</span> <span class="o">&gt;=</span> <span class="mi">50000</span><span class="p">):</span>
            <span class="n">use_fp16</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Forcing FP16 for large dataset (</span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s2"> samples, </span><span class="si">{</span><span class="n">n_dims</span><span class="si">}</span><span class="s2">D)&quot;</span><span class="p">)</span>
        
        <span class="c1"># Compute optimal chunk size based on available memory</span>
        <span class="k">if</span> <span class="n">chunk_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_optimal_chunk_size</span><span class="p">(</span>
                <span class="n">n_samples</span><span class="p">,</span>
                <span class="n">n_dims</span><span class="p">,</span>
                <span class="n">operation_type</span><span class="o">=</span><span class="s2">&quot;knn&quot;</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">use_fp16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory-efficient k-NN: chunk_size=</span><span class="si">{</span><span class="n">chunk_size</span><span class="si">}</span><span class="s2">, FP16=</span><span class="si">{</span><span class="n">use_fp16</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
        <span class="c1"># Call parent method with our settings</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_compute_knn</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">use_fp16</span><span class="o">=</span><span class="n">use_fp16</span><span class="p">)</span>
        
        <span class="c1"># Aggressive memory cleanup</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_compute_forces</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">positions</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">max_iterations</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute forces with memory-efficient strategies and PyKeOps integration.</span>
<span class="sd">        </span>
<span class="sd">        This method overrides the parent force computation with enhanced memory</span>
<span class="sd">        management, point-by-point fallback capabilities, and optional PyKeOps</span>
<span class="sd">        LazyTensors for exact repulsion computation.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        positions : torch.Tensor</span>
<span class="sd">            Current positions of points in embedding space, shape (n_samples, n_components).</span>
<span class="sd">        iteration : int</span>
<span class="sd">            Current iteration number (0-indexed).</span>
<span class="sd">        max_iterations : int</span>
<span class="sd">            Total number of iterations planned.</span>
<span class="sd">        chunk_size : int, optional</span>
<span class="sd">            Maximum chunk size for processing. If None, automatically computed</span>
<span class="sd">            based on available memory.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Computed forces of shape (n_samples, n_components).</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Private method, should not be called directly. Used by _optimize_layout().</span>
<span class="sd">        </span>
<span class="sd">        Force Computation Strategy:</span>
<span class="sd">        - **Attraction forces**: Vectorized computation with fallback to point-by-point</span>
<span class="sd">        - **Repulsion forces**: Chooses between PyKeOps LazyTensors, exact computation,</span>
<span class="sd">          or chunked random sampling based on dataset size and available memory</span>
<span class="sd">        - **Memory management**: Aggressive cleanup of intermediate tensors</span>
<span class="sd">        </span>
<span class="sd">        Backend Selection for Repulsion:</span>
<span class="sd">        - PyKeOps LazyTensors: For datasets &lt; pykeops_threshold on GPU</span>
<span class="sd">        - Exact computation: When use_exact_repulsion=True (testing only)</span>
<span class="sd">        - Random sampling: For large datasets or memory-constrained environments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">positions</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">forces</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        
        <span class="c1"># Auto-adjust chunk size based on available memory</span>
        <span class="k">if</span> <span class="n">chunk_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">chunk_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_optimal_chunk_size</span><span class="p">(</span>
                <span class="n">n_samples</span><span class="p">,</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">n_components</span><span class="p">,</span> 
                <span class="n">operation_type</span><span class="o">=</span><span class="s2">&quot;repulsion&quot;</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_fp16</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>
        
        <span class="c1"># Linear cooling</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">iteration</span> <span class="o">/</span> <span class="n">max_iterations</span>
        
        <span class="c1"># Parameters</span>
        <span class="n">a_val</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_a</span><span class="p">)</span>
        <span class="n">b_val</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_b</span><span class="p">)</span>
        <span class="n">b_exp</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">b_val</span><span class="p">)</span>
        
        <span class="c1"># ============ ATTRACTION FORCES (vectorized for speed) ============</span>
        <span class="c1"># Use cached tensor to avoid repeated CPU-&gt;GPU transfer each iteration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_knn_indices_torch&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_knn_indices_torch</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">positions</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_knn_indices_torch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_knn_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">positions</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">knn_indices_torch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_knn_indices_torch</span>
        <span class="c1"># Use compiled kernel for fused attraction forces</span>
        <span class="n">attraction_forces</span> <span class="o">=</span> <span class="n">_attraction_forces_compiled</span><span class="p">(</span>
            <span class="n">positions</span><span class="p">,</span> <span class="n">positions</span><span class="p">,</span> <span class="n">knn_indices_torch</span><span class="p">,</span> <span class="n">a_val</span><span class="p">,</span> <span class="n">b_exp</span>
        <span class="p">)</span>
        <span class="n">forces</span> <span class="o">+=</span> <span class="n">attraction_forces</span>
        
        <span class="c1"># ============ REPULSION FORCES ============</span>
        <span class="c1"># Decide whether to use PyKeOps based on dataset size and availability</span>
        <span class="n">use_pykeops</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">PYKEOPS_AVAILABLE</span> <span class="ow">and</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">use_pykeops_repulsion</span> <span class="ow">and</span> 
            <span class="n">n_samples</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">pykeops_threshold</span> <span class="ow">and</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">and</span>
            <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_exact_repulsion</span>  <span class="c1"># Don&#39;t use if exact repulsion is requested</span>
        <span class="p">)</span>
        
        <span class="k">if</span> <span class="n">use_pykeops</span><span class="p">:</span>
            <span class="c1"># Use PyKeOps LazyTensors for efficient all-pairs repulsion</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Using PyKeOps LazyTensors for repulsion&quot;</span><span class="p">)</span>

            <span class="c1"># Ensure contiguity for PyKeOps</span>
            <span class="n">X_i</span> <span class="o">=</span> <span class="n">LazyTensor</span><span class="p">(</span><span class="n">positions</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>  <span class="c1"># (N, 1, D)</span>
            <span class="n">X_j</span> <span class="o">=</span> <span class="n">LazyTensor</span><span class="p">(</span><span class="n">positions</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">())</span>  <span class="c1"># (1, N, D)</span>
            
            <span class="c1"># Compute differences and distances</span>
            <span class="n">diff</span> <span class="o">=</span> <span class="n">X_j</span> <span class="o">-</span> <span class="n">X_i</span>  <span class="c1"># (N, N, D) lazy</span>
            <span class="n">D_ij</span> <span class="o">=</span> <span class="p">((</span><span class="n">diff</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">+</span> <span class="mf">1e-10</span>  <span class="c1"># (N, N) lazy</span>
            
            <span class="c1"># Repulsion kernel</span>
            <span class="n">rep_kernel</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">a_val</span> <span class="o">*</span> <span class="p">(</span><span class="n">D_ij</span> <span class="o">**</span> <span class="n">b_exp</span><span class="p">))</span>
            
            <span class="c1"># Apply cutoff</span>
            <span class="n">cutoff_scale</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">D_ij</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
            <span class="n">rep_kernel</span> <span class="o">=</span> <span class="n">rep_kernel</span> <span class="o">*</span> <span class="n">cutoff_scale</span>
            
            <span class="c1"># Compute forces (reduction happens efficiently in PyKeOps)</span>
            <span class="c1"># For LazyTensors, division broadcasts automatically</span>
            <span class="n">force_dir</span> <span class="o">=</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">D_ij</span>
            <span class="n">rep_forces</span> <span class="o">=</span> <span class="p">(</span><span class="n">rep_kernel</span> <span class="o">*</span> <span class="n">force_dir</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">forces</span> <span class="o">+=</span> <span class="n">rep_forces</span>
            
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_exact_repulsion</span><span class="p">:</span>
            <span class="c1"># Use exact all-pairs repulsion (memory intensive, for testing)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Using exact all-pairs repulsion (memory intensive)&quot;</span><span class="p">)</span>
            <span class="c1"># Fall back to parent implementation</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">_compute_forces</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="n">max_iterations</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Use chunked random sampling to avoid memory issues with large datasets</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Using chunked random sampling for repulsion&quot;</span><span class="p">)</span>
            <span class="n">n_neg</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">neg_ratio</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_neighbors</span><span class="p">),</span> <span class="n">n_samples</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Process repulsion in chunks to avoid large tensor allocation</span>
            <span class="n">repulsion_chunk_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
            
            <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">repulsion_chunk_size</span><span class="p">):</span>
                <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start_idx</span> <span class="o">+</span> <span class="n">repulsion_chunk_size</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
                <span class="n">chunk_size_actual</span> <span class="o">=</span> <span class="n">end_idx</span> <span class="o">-</span> <span class="n">start_idx</span>
                
                <span class="c1"># Generate negative samples for this chunk only</span>
                <span class="n">neg_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="p">(</span><span class="n">chunk_size_actual</span><span class="p">,</span> <span class="n">n_neg</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                
                <span class="c1"># Avoid self-selection more memory efficiently</span>
                <span class="c1"># Create arange only for chunk size</span>
                <span class="n">chunk_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">start_idx</span><span class="p">,</span> <span class="n">end_idx</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">neg_indices</span> <span class="o">==</span> <span class="n">chunk_indices</span>
                
                <span class="c1"># Replace self-indices with random alternatives (avoid large tensor creation)</span>
                <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                    <span class="c1"># Generate replacement indices on-the-fly</span>
                    <span class="n">replacement_base</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="p">(</span><span class="n">chunk_size_actual</span><span class="p">,</span> <span class="n">n_neg</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="c1"># Ensure replacements are different from self</span>
                    <span class="n">replacement_mask</span> <span class="o">=</span> <span class="n">replacement_base</span> <span class="o">==</span> <span class="n">chunk_indices</span>
                    <span class="k">while</span> <span class="n">replacement_mask</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                        <span class="n">replacement_base</span><span class="p">[</span><span class="n">replacement_mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> 
                                                                         <span class="p">(</span><span class="n">replacement_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(),),</span> 
                                                                         <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                        <span class="n">replacement_mask</span> <span class="o">=</span> <span class="n">replacement_base</span> <span class="o">==</span> <span class="n">chunk_indices</span>
                    
                    <span class="n">neg_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">replacement_base</span><span class="p">,</span> <span class="n">neg_indices</span><span class="p">)</span>
                
                <span class="c1"># Compute repulsion for this chunk</span>
                <span class="n">chunk_positions</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
                <span class="n">neg_positions</span> <span class="o">=</span> <span class="n">positions</span><span class="p">[</span><span class="n">neg_indices</span><span class="p">]</span>  <span class="c1"># shape: (chunk_size, n_neg, n_components)</span>
                <span class="n">center_positions</span> <span class="o">=</span> <span class="n">chunk_positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape: (chunk_size, 1, n_components)</span>
                
                <span class="c1"># Compute differences and distances</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="n">neg_positions</span> <span class="o">-</span> <span class="n">center_positions</span>
                <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">diff</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-10</span>
                
                <span class="c1"># Repulsion coefficients</span>
                <span class="n">rep_coeff</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">a_val</span> <span class="o">*</span> <span class="p">(</span><span class="n">dist</span> <span class="o">**</span> <span class="n">b_exp</span><span class="p">))</span>
                <span class="n">cutoff_scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">dist</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">)</span>
                <span class="n">rep_coeff</span> <span class="o">=</span> <span class="n">rep_coeff</span> <span class="o">*</span> <span class="n">cutoff_scale</span>
                
                <span class="c1"># Sum repulsion forces for this chunk</span>
                <span class="n">chunk_repulsion_forces</span> <span class="o">=</span> <span class="p">(</span><span class="n">rep_coeff</span> <span class="o">*</span> <span class="n">diff</span> <span class="o">/</span> <span class="n">dist</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">forces</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">chunk_repulsion_forces</span>
                
                <span class="c1"># Clear intermediate tensors to free memory</span>
                <span class="k">del</span> <span class="n">neg_indices</span><span class="p">,</span> <span class="n">neg_positions</span><span class="p">,</span> <span class="n">diff</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">rep_coeff</span><span class="p">,</span> <span class="n">cutoff_scale</span><span class="p">,</span> <span class="n">chunk_repulsion_forces</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        
        <span class="c1"># Apply cooling and clipping</span>
        <span class="n">forces</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">forces</span>
        <span class="n">forces</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">forces</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">forces</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">_optimize_layout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_positions</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Optimize embedding layout with enhanced memory monitoring and management.</span>
<span class="sd">        </span>
<span class="sd">        This method overrides the parent optimization loop with real-time memory</span>
<span class="sd">        monitoring, more frequent cleanup, and detailed progress reporting.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        initial_positions : torch.Tensor</span>
<span class="sd">            Initial embedding positions of shape (n_samples, n_components).</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            Optimized final positions of shape (n_samples, n_components),</span>
<span class="sd">            normalized to zero mean and unit standard deviation.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Private method, should not be called directly. Used by fit_transform().</span>
<span class="sd">        </span>
<span class="sd">        Enhancements over parent method:</span>
<span class="sd">        - Real-time GPU memory monitoring and warnings</span>
<span class="sd">        - More frequent cache clearing (every 10 iterations)</span>
<span class="sd">        - Detailed memory usage reporting</span>
<span class="sd">        - Low memory warnings when free GPU memory &lt; 2GB</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">initial_positions</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory-efficient optimization for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="si">}</span><span class="s2"> points...&quot;</span><span class="p">)</span>
        
        <span class="c1"># Log initial memory usage</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="n">mem_reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
            <span class="n">mem_total</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">total_memory</span> <span class="o">/</span> <span class="mf">1e9</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Initial GPU memory: </span><span class="si">{</span><span class="n">mem_reserved</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">GB used / </span><span class="si">{</span><span class="n">mem_total</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB total&quot;</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter_layout</span><span class="p">):</span>
            <span class="c1"># Monitor memory before computation</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mem_available</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mf">1e9</span>
                <span class="n">mem_reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>

                <span class="c1"># Warn if memory usage is getting high</span>
                <span class="k">if</span> <span class="n">mem_available</span> <span class="o">&lt;</span> <span class="mf">2.0</span><span class="p">:</span>  <span class="c1"># Less than 2GB free</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Low GPU memory: </span><span class="si">{</span><span class="n">mem_available</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB free, </span><span class="si">{</span><span class="n">mem_reserved</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB used&quot;</span><span class="p">)</span>
            
            <span class="c1"># Compute forces with our memory-efficient method</span>
            <span class="n">forces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_forces</span><span class="p">(</span><span class="n">positions</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter_layout</span><span class="p">)</span>
            
            <span class="c1"># Update positions</span>
            <span class="n">positions</span> <span class="o">+=</span> <span class="n">forces</span>
            
            <span class="c1"># More frequent logging and memory management</span>
            <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">%</span> <span class="mi">20</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">force_mag</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">forces</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
                        <span class="n">mem_reserved</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1e9</span>
                        <span class="n">mem_available</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">mem_get_info</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="mf">1e9</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter_layout</span><span class="si">}</span><span class="s2">, avg force: </span><span class="si">{</span><span class="n">force_mag</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">, &quot;</span>
                                       <span class="sa">f</span><span class="s2">&quot;GPU memory: </span><span class="si">{</span><span class="n">mem_reserved</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB used, </span><span class="si">{</span><span class="n">mem_available</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">GB free&quot;</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Iteration </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_iter_layout</span><span class="si">}</span><span class="s2">, avg force: </span><span class="si">{</span><span class="n">force_mag</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                
                <span class="c1"># Aggressive memory cleanup</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        
        <span class="c1"># Final normalization</span>
        <span class="n">positions</span> <span class="o">-=</span> <span class="n">positions</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">positions</span> <span class="o">/=</span> <span class="n">positions</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">positions</span>
    
<div class="viewcode-block" id="DiRePyTorchMemoryEfficient.fit_transform">
<a class="viewcode-back" href="../../api/modules.html#dire_rapids.dire_pytorch_memory_efficient.DiRePyTorchMemoryEfficient.fit_transform">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fit the model and transform data with memory-efficient processing.</span>
<span class="sd">        </span>
<span class="sd">        This method extends the parent implementation with memory-optimized data</span>
<span class="sd">        handling, enhanced logging, and aggressive cleanup procedures.</span>
<span class="sd">        </span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            High-dimensional input data to transform.</span>
<span class="sd">        y : array-like of shape (n_samples,), optional</span>
<span class="sd">            Ignored. Present for scikit-learn API compatibility.</span>
<span class="sd">            </span>
<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        numpy.ndarray of shape (n_samples, n_components)</span>
<span class="sd">            Low-dimensional embedding of the input data.</span>
<span class="sd">            </span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Memory Optimizations:</span>
<span class="sd">        - Automatic FP16 conversion for large datasets on GPU</span>
<span class="sd">        - Strategic backend selection based on dataset characteristics</span>
<span class="sd">        - Aggressive memory cleanup after processing</span>
<span class="sd">        - Real-time memory monitoring and reporting</span>
<span class="sd">        </span>
<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Process large dataset with memory monitoring::</span>
<span class="sd">        </span>
<span class="sd">            import numpy as np</span>
<span class="sd">            from dire_rapids import DiRePyTorchMemoryEfficient</span>
<span class="sd">            </span>
<span class="sd">            # Large high-dimensional dataset</span>
<span class="sd">            X = np.random.randn(200000, 1000)</span>
<span class="sd">            </span>
<span class="sd">            reducer = DiRePyTorchMemoryEfficient(</span>
<span class="sd">                use_fp16=True,</span>
<span class="sd">                memory_fraction=0.3,</span>
<span class="sd">                verbose=True</span>
<span class="sd">            )</span>
<span class="sd">            </span>
<span class="sd">            embedding = reducer.fit_transform(X)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Store data (always float32 for CPU operations; fp16 conversion</span>
        <span class="c1"># happens on-the-fly in GPU kernels when use_fp16 is set).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Memory-efficient processing: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="si">}</span><span class="s2"> samples, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2"> features&quot;</span><span class="p">)</span>
        
        <span class="c1"># Provide strategy information</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">pykeops_threshold</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Large dataset (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_samples</span><span class="si">}</span><span class="s2"> &gt; </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">pykeops_threshold</span><span class="si">}</span><span class="s2">): using random sampling for repulsion&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">PYKEOPS_AVAILABLE</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_pykeops_repulsion</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using PyKeOps LazyTensors for exact repulsion (memory efficient)&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using random sampling for repulsion&quot;</span><span class="p">)</span>
        
        <span class="c1"># Call parent fit_transform</span>
        <span class="n">result</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># Extra aggressive cleanup</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
        <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">result</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Alexander Kolpakov (UATX), Igor Rivin (Temple University).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>